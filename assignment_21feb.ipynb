{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d99d41a-d548-4b13-a0c1-0977103a1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename=\"21_feb.log\",level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79226cf7-aafa-479c-8f89-a1ab86595dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nData extraction from websites is done automatically using a method called web scraping.\\n\\n\\nThere are several uses for web scraping:\\n\\nOnline scraping is frequently used to gather copious volumes of data from websites.\\nThis might be helpful in any circumstance where access to a lot of data is needed, \\nsuch as market research, lead creation, competition analysis, etc.\\n\\nWeb scraping is used to gather data in a number of specific fields, including:\\n\\nE-commerce: From e-commerce websites, product details, costs, and reviews are extracted via web scraping.\\nRetailers might use this data to keep an eye on their rivals, change their pricing policies, or broaden their selection of goods.\\n\\nWeb scraping is a technique used in finance to gather financial information from websites like stock exchanges,\\nnews portals, and financial blogs.\\nInvestors can use this information to make well-informed decisions about their investments.\\n\\nResearch: Researchers utilise web scraping to gather information from a range of sources,\\nincluding social media, news websites, and online discussion forums.\\nThis information can be utilised to research patterns, keep tabs on popular opinion, or examine user activity.\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ans no1\n",
    "\n",
    "logging.info(\"Web Scraping , Uses, three areas where Web Scraping is used to get data\")\n",
    "\n",
    "'''\n",
    "Data extraction from websites is done automatically using a method called web scraping.\n",
    "\n",
    "\n",
    "There are several uses for web scraping:\n",
    "\n",
    "Online scraping is frequently used to gather copious volumes of data from websites.\n",
    "This might be helpful in any circumstance where access to a lot of data is needed, \n",
    "such as market research, lead creation, competition analysis, etc.\n",
    "\n",
    "Web scraping is used to gather data in a number of specific fields, including:\n",
    "\n",
    "E-commerce: From e-commerce websites, product details, costs, and reviews are extracted via web scraping.\n",
    "Retailers might use this data to keep an eye on their rivals, change their pricing policies, or broaden their selection of goods.\n",
    "\n",
    "Web scraping is a technique used in finance to gather financial information from websites like stock exchanges,\n",
    "news portals, and financial blogs.\n",
    "Investors can use this information to make well-informed decisions about their investments.\n",
    "\n",
    "Research: Researchers utilise web scraping to gather information from a range of sources,\n",
    "including social media, news websites, and online discussion forums.\n",
    "This information can be utilised to research patterns, keep tabs on popular opinion, or examine user activity.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5100f0-5978-4825-8dbb-2f24a8069910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThere are several methods used for web scraping, each with its own advantages and disadvantages:\\n\\nThe simplest method of web scraping is manual copy and paste, \\nwhich entails physically copying data from websites and inserting it into a spreadsheet or other document. \\nAlthough though it is a difficult and slow procedure, it works well for modest amounts of data.\\n\\nTools for Online Scraping: There are numerous tools for web scraping that can automate the process of extracting data from websites. \\nThese programmes can be either free or expensive and often require some level of coding expertise.\\nExamples are Selenium, Scrapy, and Lovely Soup.\\n\\nAPIs: Several websites offer application programming interfaces (APIs), \\nwhich let programmers access organised data. Because the data is provided directly by the website owner, \\nthis method is quicker and more dependable than web scraping.\\n\\nWeb crawlers are programmes that visit websites automatically and collect data from them. \\nLarge volumes of data can be scraped using this method,\\nalthough it can be resource-intensive and setup may need some technical know-how.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ans no2\n",
    "\n",
    "logging.info(\" different methods used for Web Scraping\")\n",
    "'''\n",
    "There are several methods used for web scraping, each with its own advantages and disadvantages:\n",
    "\n",
    "The simplest method of web scraping is manual copy and paste, \n",
    "which entails physically copying data from websites and inserting it into a spreadsheet or other document. \n",
    "Although though it is a difficult and slow procedure, it works well for modest amounts of data.\n",
    "\n",
    "Tools for Online Scraping: There are numerous tools for web scraping that can automate the process of extracting data from websites. \n",
    "These programmes can be either free or expensive and often require some level of coding expertise.\n",
    "Examples are Selenium, Scrapy, and Lovely Soup.\n",
    "\n",
    "APIs: Several websites offer application programming interfaces (APIs), \n",
    "which let programmers access organised data. Because the data is provided directly by the website owner, \n",
    "this method is quicker and more dependable than web scraping.\n",
    "\n",
    "Web crawlers are programmes that visit websites automatically and collect data from them. \n",
    "Large volumes of data can be scraped using this method,\n",
    "although it can be resource-intensive and setup may need some technical know-how.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71917d05-a82d-449c-ae5c-bdae9703c272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe paraphraser in QuillBot uses your sentences and modifies them to help you rework and revise them.\\nA Python module called Beautiful Soup is employed for web scraping. \\nBecause of its simplicity, use, and versatility, it is a well-liked library among developers. \\nA potent tool for web scraping, Beautiful Soup may be used to extract data from HTML and XML files.\\n\\nThe following are some of the factors that make Beautiful Soup popular for web scraping:\\n\\nBeautiful Soup is easy to use, making it ideal for developers with varying levels of experience.\\n\\nThe library can handle complex parsing scenarios, such as nested tags and irregular markup.\\n\\nIt is versatile and can be customized to handle different parsing scenarios.\\n\\nBeautiful Soup can be easily integrated with other Python libraries, \\nmaking it a powerful tool for web scraping and data analysis workflows.\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ans no3\n",
    "\n",
    "logging.info(\" Beautiful Soup,it's uses\")\n",
    "'''\n",
    "The paraphraser in QuillBot uses your sentences and modifies them to help you rework and revise them.\n",
    "A Python module called Beautiful Soup is employed for web scraping. \n",
    "Because of its simplicity, use, and versatility, it is a well-liked library among developers. \n",
    "A potent tool for web scraping, Beautiful Soup may be used to extract data from HTML and XML files.\n",
    "\n",
    "The following are some of the factors that make Beautiful Soup popular for web scraping:\n",
    "\n",
    "Beautiful Soup is easy to use, making it ideal for developers with varying levels of experience.\n",
    "\n",
    "The library can handle complex parsing scenarios, such as nested tags and irregular markup.\n",
    "\n",
    "It is versatile and can be customized to handle different parsing scenarios.\n",
    "\n",
    "Beautiful Soup can be easily integrated with other Python libraries, \n",
    "making it a powerful tool for web scraping and data analysis workflows.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2483708-9be1-4448-b3a9-f6bcb94cc042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFlask is a web framework used for building web applications with Python.\\n\\nIt is lightweight and flexible, making it easy to create simple and complex web applications.\\n\\nFlask provides a simple API for handling HTTP requests and responses, \\nallowing developers to quickly create web applications.\\n\\nFlask is often used in web scraping projects to create a web interface for displaying scraped data to users.\\n\\nThe web interface can be used to visualize the results of the scraping process\\nand make it easier for users to interact with the data.\\n\\nFlask is modular, which means it can be easily extended with third-party libraries and plugins, \\nmaking it a powerful tool for building complex web scraping applications.\\n\\nFlask is a popular choice for web scraping projects due to its simplicity, flexibility, and ease of use.\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ans no4\n",
    "\n",
    "\n",
    "logging.info(\"flask used in this Web Scraping project\")\n",
    "\n",
    "'''\n",
    "Flask is a web framework used for building web applications with Python.\n",
    "\n",
    "It is lightweight and flexible, making it easy to create simple and complex web applications.\n",
    "\n",
    "Flask provides a simple API for handling HTTP requests and responses, \n",
    "allowing developers to quickly create web applications.\n",
    "\n",
    "Flask is often used in web scraping projects to create a web interface for displaying scraped data to users.\n",
    "\n",
    "The web interface can be used to visualize the results of the scraping process\n",
    "and make it easier for users to interact with the data.\n",
    "\n",
    "Flask is modular, which means it can be easily extended with third-party libraries and plugins, \n",
    "making it a powerful tool for building complex web scraping applications.\n",
    "\n",
    "Flask is a popular choice for web scraping projects due to its simplicity, flexibility, and ease of use.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc7df5f-fbdd-4b9b-a469-a8a973eb2b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEC2 (Elastic Compute Cloud): EC2 is used to provision and manage virtual servers in the cloud.\\nIn this project, EC2 is used to host the web application and run the web scraping script.\\n\\nS3 (Simple Storage Service): S3 is a cloud-based object storage service used to store and retrieve data. \\nIn this project, S3 is used to store the scraped data in a CSV file.\\n\\nIAM (Identity and Access Management): IAM is used to manage access to AWS services and resources. \\nIn this project, IAM is used to create a new user with limited permissions to access the S3 bucket.\\n\\nCloudWatch: CloudWatch is a monitoring and logging service used to monitor and analyze system and application logs.\\nIn this project, CloudWatch is used to monitor the performance of the EC2 instance running the web application and the web scraping script.\\n\\nRoute 53: Route 53 is a domain name system (DNS) service used to route internet traffic to web applications.\\nIn this project, Route 53 is used to map the domain name to the IP address of the EC2 instance hosting the web application.\\n\\nOverall, these AWS services are used to provision, monitor, and store the data for the web scraping project. \\nThey provide a secure and scalable infrastructure for running the web application and storing the scraped data in the cloud.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ans no5\n",
    "\n",
    "logging.info(\"AWS services used in this project use of each service\")\n",
    "\n",
    "'''\n",
    "EC2 (Elastic Compute Cloud): EC2 is used to provision and manage virtual servers in the cloud.\n",
    "In this project, EC2 is used to host the web application and run the web scraping script.\n",
    "\n",
    "S3 (Simple Storage Service): S3 is a cloud-based object storage service used to store and retrieve data. \n",
    "In this project, S3 is used to store the scraped data in a CSV file.\n",
    "\n",
    "IAM (Identity and Access Management): IAM is used to manage access to AWS services and resources. \n",
    "In this project, IAM is used to create a new user with limited permissions to access the S3 bucket.\n",
    "\n",
    "CloudWatch: CloudWatch is a monitoring and logging service used to monitor and analyze system and application logs.\n",
    "In this project, CloudWatch is used to monitor the performance of the EC2 instance running the web application and the web scraping script.\n",
    "\n",
    "Route 53: Route 53 is a domain name system (DNS) service used to route internet traffic to web applications.\n",
    "In this project, Route 53 is used to map the domain name to the IP address of the EC2 instance hosting the web application.\n",
    "\n",
    "Overall, these AWS services are used to provision, monitor, and store the data for the web scraping project. \n",
    "They provide a secure and scalable infrastructure for running the web application and storing the scraped data in the cloud.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00ca8f-b3a9-4d69-8e91-6d9403bbfd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
