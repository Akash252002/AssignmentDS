{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ans no1\n",
        "# A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
        "\n",
        "\n",
        "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem:\n",
        "\n",
        "P(smoker | uses insurance plan) = P(uses insurance plan | smoker) * P(smoker) / P(uses insurance plan)\n",
        "\n",
        "We are given that P(uses insurance plan) = 0.7, P(smoker) = 0.4, and P(uses insurance plan | smoker) = 1 (since all smokers use the insurance plan).\n",
        "\n",
        "To calculate P(uses insurance plan), we can use the law of total probability:\n",
        "\n",
        "P(uses insurance plan) = P(uses insurance plan | smoker) * P(smoker) + P(uses insurance plan | non-smoker) * P(non-smoker)\n",
        "= 1 * 0.4 + P(uses insurance plan | non-smoker) * 0.6\n",
        "\n",
        "We are not given the value of P(uses insurance plan | non-smoker), but we can assume it to be lower than 1 (since not all non-smokers may choose to use the insurance plan). Let's assume P(uses insurance plan | non-smoker) = 0.2.\n",
        "\n",
        "Then, we can calculate:\n",
        "\n",
        "P(uses insurance plan) = 1 * 0.4 + 0.2 * 0.6 = 0.46\n",
        "\n",
        "Now we can plug in all the values into Bayes' theorem:\n",
        "\n",
        "P(smoker | uses insurance plan) = 1 * 0.4 / 0.7 = 0.57\n",
        "\n",
        "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.57, or 57%."
      ],
      "metadata": {
        "id": "3zY2JbXUjKCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans no2\n",
        "#What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
        "\n",
        "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes algorithm used in machine learning for classification tasks. The main difference between them lies in the type of data they are designed to handle.\n",
        "\n",
        "Bernoulli Naive Bayes is used for binary classification problems, where each feature is either present or absent. This type of data is often called \"boolean\" or \"binomial\". For example, in a spam classification problem, each feature could represent the presence or absence of a certain word in an email.\n",
        "\n",
        "Multinomial Naive Bayes, on the other hand, is used for classification problems where the features represent counts or frequencies. This type of data is often called \"multinomial\". For example, in a text classification problem, each feature could represent the frequency of a certain word in a document.\n",
        "\n",
        "The two algorithms differ in their assumptions about the distribution of the data. Bernoulli Naive Bayes assumes that the features are binary and follows a Bernoulli distribution, while Multinomial Naive Bayes assumes that the features are counts or frequencies and follows a multinomial distribution.\n",
        "\n",
        "In practice, Bernoulli Naive Bayes is often used for text classification tasks such as sentiment analysis or spam filtering, while Multinomial Naive Bayes is used for problems such as document classification or topic modeling. However, the choice between the two algorithms ultimately depends on the nature of the data and the specific problem at hand.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K8lulPXQjMfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans no3\n",
        "# How does Bernoulli Naive Bayes handle missing values?\n",
        "\n",
        "Bernoulli Naive Bayes is a variant of the Naive Bayes algorithm used for binary classification problems. In Bernoulli Naive Bayes, each feature is either present or absent, and follows a Bernoulli distribution. Missing values can be a problem when working with this type of data.\n",
        "\n",
        "There are different approaches to handling missing values in Bernoulli Naive Bayes, depending on the specific problem and the amount of missing data.\n",
        "\n",
        "One common approach is to impute missing values with a default value, such as 0 or 1, depending on whether the feature is more likely to be present or absent. This approach assumes that the missing values are missing at random, meaning that the probability of a value being missing is not related to the value itself or to other variables. However, this assumption may not hold in all cases, and imputing missing values can lead to biased or inaccurate results.\n",
        "\n",
        "Another approach is to treat missing values as a separate category, and add a new feature to the model to represent this category. This approach assumes that the missing values are not missing at random, but rather represent a distinct category that is different from both present and absent values. However, this approach can lead to an increase in the dimensionality of the data and may require more training data to achieve good results.\n",
        "\n",
        "A third approach is to use advanced techniques such as matrix completion or probabilistic matrix factorization to estimate the missing values based on the observed data. These techniques can be more complex and computationally expensive, but can lead to more accurate results when dealing with large amounts of missing data.\n",
        "\n",
        "In general, the choice of approach depends on the nature of the data and the specific problem at hand. It is important to carefully consider the assumptions and limitations of each approach, and to evaluate the performance of the model on both training and test data to ensure that it is robust and accurate.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XKoSnWJqjQfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ans no4\n",
        "#Can Gaussian Naive Bayes be used for multi-class classification?\n",
        "\n",
        "Yes, Gaussian Naive Bayes can be used for multi-class classification problems. In fact, Gaussian Naive Bayes is a commonly used algorithm for multi-class classification tasks.\n",
        "\n",
        "To use Gaussian Naive Bayes for multi-class classification, the algorithm is typically trained on a dataset with multiple classes, each represented by a set of features. During training, the algorithm estimates the parameters of a Gaussian distribution for each class and each feature. The parameters include the mean and variance of the distribution.\n",
        "\n",
        "During prediction, the algorithm computes the posterior probability of each class given the input features, using Bayes' theorem. The class with the highest posterior probability is then predicted as the output.\n",
        "\n",
        "One important consideration when using Gaussian Naive Bayes for multi-class classification is the assumption of independence between features. This assumption can be more difficult to maintain when dealing with multiple classes and complex feature interactions. However, in practice, Gaussian Naive Bayes can still be effective for many multi-class classification problems, particularly those with relatively simple and independent features.\n",
        "\n",
        "In cases where the features are not independent, other variants of Naive Bayes, such as the Multinomial Naive Bayes or the Bernoulli Naive Bayes, may be more appropriate for multi-class classification. Additionally, other machine learning algorithms, such as decision trees or support vector machines, may be better suited for certain types of multi-class classification problems."
      ],
      "metadata": {
        "id": "2Dal3p66jYmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans no5\n",
        "# Data preparation:\n",
        "import pandas as pd\n",
        "\n",
        "# Set the file path and delimiter\n",
        "file_path = '/content/spambase.data'\n",
        "\n",
        "# Read the file into a pandas DataFrame\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Print the first few rows of the DataFrame to check if the data was read correctly\n",
        "print(data.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7v4sWVLlQh-",
        "outputId": "33266ad6-f64e-4bfe-80b7-a52c405d3587"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0  0.64  0.64.1  0.1  0.32   0.2   0.3   0.4   0.5   0.6  ...  0.41  \\\n",
            "0  0.21  0.28    0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
            "1  0.06  0.00    0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
            "2  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
            "3  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
            "4  0.00  0.00    0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00  ...  0.00   \n",
            "\n",
            "    0.42  0.43  0.778   0.44   0.45  3.756   61   278  1  \n",
            "0  0.132   0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
            "1  0.143   0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
            "2  0.137   0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
            "3  0.135   0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
            "4  0.223   0.0  0.000  0.000  0.000  3.000   15    54  1  \n",
            "\n",
            "[5 rows x 58 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation:\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import pandas as pd\n",
        "\n",
        "# Split the data into input features (X) and target variable (y)\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Initialize the three classifiers\n",
        "bnb = BernoulliNB()\n",
        "mnb = MultinomialNB()\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Perform 10-fold cross-validation for each classifier\n",
        "bnb_scores = cross_val_score(bnb, X, y, cv=10)\n",
        "mnb_scores = cross_val_score(mnb, X, y, cv=10)\n",
        "gnb_scores = cross_val_score(gnb, X, y, cv=10)\n",
        "\n",
        "# Print the mean accuracy and standard deviation for each classifier\n",
        "print(\"Bernoulli Naive Bayes accuracy: %0.2f (+/- %0.2f)\" % (bnb_scores.mean(), bnb_scores.std() * 2))\n",
        "print(\"Multinomial Naive Bayes accuracy: %0.2f (+/- %0.2f)\" % (mnb_scores.mean(), mnb_scores.std() * 2))\n",
        "print(\"Gaussian Naive Bayes accuracy: %0.2f (+/- %0.2f)\" % (gnb_scores.mean(), gnb_scores.std() * 2))\n"
      ],
      "metadata": {
        "id": "agQ6wD0Do7SD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7caca9-2e04-4078-b83c-cb9636130505"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bernoulli Naive Bayes accuracy: 0.88 (+/- 0.09)\n",
            "Multinomial Naive Bayes accuracy: 0.79 (+/- 0.08)\n",
            "Gaussian Naive Bayes accuracy: 0.82 (+/- 0.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Results:\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "\n",
        "# Perform 10-fold cross-validation for each classifier\n",
        "bnb_pred = cross_val_predict(bnb, X, y, cv=10)\n",
        "mnb_pred = cross_val_predict(mnb, X, y, cv=10)\n",
        "gnb_pred = cross_val_predict(gnb, X, y, cv=10)\n",
        "\n",
        "# Calculate performance metrics for each classifier\n",
        "bnb_acc = accuracy_score(y, bnb_pred)\n",
        "bnb_prec = precision_score(y, bnb_pred)\n",
        "bnb_rec = recall_score(y, bnb_pred)\n",
        "bnb_f1 = f1_score(y, bnb_pred)\n",
        "\n",
        "mnb_acc = accuracy_score(y, mnb_pred)\n",
        "mnb_prec = precision_score(y, mnb_pred)\n",
        "mnb_rec = recall_score(y, mnb_pred)\n",
        "mnb_f1 = f1_score(y, mnb_pred)\n",
        "\n",
        "gnb_acc = accuracy_score(y, gnb_pred)\n",
        "gnb_prec = precision_score(y, gnb_pred)\n",
        "gnb_rec = recall_score(y, gnb_pred)\n",
        "gnb_f1 = f1_score(y, gnb_pred)\n",
        "\n",
        "# Print performance metrics for each classifier\n",
        "print(\"Bernoulli Naive Bayes:\")\n",
        "print(\"Accuracy: %0.2f\" % bnb_acc)\n",
        "print(\"Precision: %0.2f\" % bnb_prec)\n",
        "print(\"Recall: %0.2f\" % bnb_rec)\n",
        "print(\"F1 score: %0.2f\" % bnb_f1)\n",
        "\n",
        "print(\"Multinomial Naive Bayes:\")\n",
        "print(\"Accuracy: %0.2f\" % mnb_acc)\n",
        "print(\"Precision: %0.2f\" % mnb_prec)\n",
        "print(\"Recall: %0.2f\" % mnb_rec)\n",
        "print(\"F1 score: %0.2f\" % mnb_f1)\n",
        "\n",
        "print(\"Gaussian Naive Bayes:\")\n",
        "print(\"Accuracy: %0.2f\" % gnb_acc)\n",
        "print(\"Precision: %0.2f\" % gnb_prec)\n",
        "print(\"Recall: %0.2f\" % gnb_rec)\n",
        "print(\"F1 score: %0.2f\" % gnb_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E1baeyL7J22",
        "outputId": "8b5fa93a-a021-467c-af0d-cf54e75d5ada"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bernoulli Naive Bayes:\n",
            "Accuracy: 0.88\n",
            "Precision: 0.88\n",
            "Recall: 0.82\n",
            "F1 score: 0.85\n",
            "Multinomial Naive Bayes:\n",
            "Accuracy: 0.79\n",
            "Precision: 0.73\n",
            "Recall: 0.72\n",
            "F1 score: 0.73\n",
            "Gaussian Naive Bayes:\n",
            "Accuracy: 0.82\n",
            "Precision: 0.70\n",
            "Recall: 0.96\n",
            "F1 score: 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Discussion:\n",
        "Based on the performance metrics reported, it appears that the Bernoulli Naive Bayes classifier performed the best with an accuracy of 0.88, precision of 0.88, recall of 0.82, and F1 score of 0.85. The Gaussian Naive Bayes classifier had the highest recall of 0.96, but a lower precision of 0.70, leading to a lower F1 score of 0.81. The Multinomial Naive Bayes classifier had the lowest performance, with an accuracy of 0.79, precision of 0.73, recall of 0.72, and F1 score of 0.73.\n",
        "\n",
        "The reason why Bernoulli Naive Bayes performed the best could be because the input features of the dataset are binary (i.e., presence or absence of certain words), which is well-suited for a Bernoulli distribution. Bernoulli Naive Bayes assumes that the input features are binary and independent of each other, which may be a reasonable assumption for the email spam classification problem. The Gaussian Naive Bayes assumes that the input features follow a Gaussian distribution, which may not be the case for the email spam dataset, leading to suboptimal performance.\n",
        "\n",
        "One limitation of Naive Bayes that can be observed is that it makes the strong assumption of independence between the input features, which may not hold in some real-world datasets. In addition, the performance of Naive Bayes can be affected by the presence of irrelevant or redundant features, which can negatively impact the accuracy of the classifier."
      ],
      "metadata": {
        "id": "SYB43w5U_HXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#conclusion:\n",
        "The best performing variant of Naive Bayes on the Spambase dataset was Bernoulli Naive Bayes, likely due to the binary nature of the input features. A limitation of Naive Bayes is its assumption of independence between input features. Future work could explore other classifiers or feature selection/engineering techniques to improve performance, and use a more diverse dataset to evaluate generalizability."
      ],
      "metadata": {
        "id": "3dYhSlJf_2nn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YaisK5fk8fxw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}